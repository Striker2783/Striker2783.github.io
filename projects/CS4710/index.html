<!DOCTYPE html>
<html>

<head>
    <title>Frequent Itemset Mining Algorithms</title>
    <link rel="stylesheet" href="/index.css" />
    <script src="/components/nav-bar.js"></script>
</head>

<body>
    <nav-bar></nav-bar>
    <main>
        <h1>Frequent Itemset Mining Algorithms</h1>
        <article>
            <section>
                <a href="https://github.com/Striker2783/data_mining/tree/master">Github Repository Link</a>
                <br>
                <a href="CS4710/paper.html">Multithreaded AprioriHybrid Paper</a>
                <h1>Introduction</h1>
                <p>This page shows a few algorithms for frequent itemset mining from a Introduction to Data Mining
                    Course I took in Spring 2025. I only needed to implement one of them.</p>
                <p>Frequent itemset mining is a process of identifying sets of items that appear frequently together in
                    a dataset. For example, if I have a dataset of shopping transactions, I can find what items were
                    commonly bought together. Using this data, I can find association rules that can be used to make
                    predictions about future behavior. Association rules determines the probability of someone buying
                    items based on what they bought previously. This is very useful in getting customers to buy more of
                    your products.</p>
            </section>
            <section>
                <h1>Algorithms</h1>
                <ul>
                    <li>
                        <a href="#apriori">Apriori</a>
                    </li>
                    <li>
                        <a href="#aprioritid">AprioriTID</a>
                    </li>
                    <li>
                        <a href="#apriorihybrid">AprioriHybrid</a>
                    </li>
                    <li>
                        <a href="#multithreaded-apriori">Multithreaded Apriori</a>
                    </li>
                    <li>
                        <a href="#multithreaded-apriorihybrid">Multithreaded AprioriHybrid</a>
                    </li>
                    <li>
                        <a href="#fp-growth">FP Growth</a>
                    </li>
                    <li>
                        <a href="#max-miner">Max Miner</a>
                    </li>
                    <li>
                        <a href="apriori-trie">Apriori with a Trie</a>
                    </li>
                </ul>
            </section>
            <section id="apriori">
                <h1>Apriori</h1>
                <p>The Apriori Algorithm is a fundamental method in the field of association rule mining, a branch of
                    data mining focused on discovering relationships among variables in large datasets. Introduced by
                    Agrawal and Srikant in 1994 titled "Fast Algorithms for Mining Association Rules in Large
                    Databases", it is designed to efficiently identify frequent itemsets and generate
                    association rules in transactional databases.
                </p>
                <p>
                    The algorithm relies on the Apriori property, also known as the downward-closure property, which
                    states that all subsets of a frequent itemset must themselves be frequent. This property allows the
                    algorithm to prune the search space efficiently: any itemset containing an infrequent subset can be
                    discarded from further consideration.
                </p>
                <p>
                    The algorithm proceeds in several iterative steps. Initially, it identifies all frequent individual
                    items that satisfy a minimum support threshold. These frequent items form the basis for generating
                    larger candidate itemsets. In each subsequent iteration, candidate itemsets of size k are generated
                    by combining frequent itemsets of size k - 1, and any candidate containing an infrequent subset is
                    pruned. The database is scanned to count the occurrences of each candidate, and those meeting the
                    minimum support requirement are retained as frequent itemsets. This iterative process continues
                    until no new frequent itemsets can be generated.
                </p>
                <p>
                    These candidate itemsets are stored in a data structure known as a hash tree. Each internal node of
                    the hash tree has an array of pointers (possibly null) to either other internal nodes or leaf nodes.
                    The leaf node contains a list of candidate itemsets and their support counts. When determining the
                    support of all the candidate itemsets, the hash tree takes in one transaction at a time and does a
                    recursive backtracking algorithm. At each internal node, it hashes each item in the transaction and
                    finds the node corresponding to that item. It then calls the recursive function on it. When it
                    reaches a leaf node, it increments the support of the itemset that was created by the items chosen.
                </p>
                <p>
                    After all frequent itemsets have been identified, the algorithm generates association rules. For
                    each frequent itemset, all possible non-empty subsets are considered, and rules are constructed by
                    treating one subset as the antecedent and the remaining items as the consequent. Each rule is
                    evaluated against a minimum confidence threshold, and only those that meet or exceed this threshold
                    are retained as strong rules.
                </p>
            </section>
            <section id="aprioritid">
                <h1>AprioriTID</h1>
                <p>The AprioriTID algorithm was also introduced by Agrawal and Srikant in the same paper as Apriori. The
                    iterative process is the same, but they used a different data structure to store candidates and the
                    database to increase performance.</p>
                <p>The data structure to store the candidate itemset is an array of objects containing details about the
                    candidate itemsets. Each element in the
                    array corresponds to a candidate itemset. The index into the array corresponding
                    to the candidate itemset is called the transaction identifier (TID). The generator field of the
                    object are the TIDs that are joined to get the current TID. The extension field are the TIDs that
                    were generated by the current TID. The support field is the support count of the candidate itemset
                    in the database. The transformed database is a list of transactions with each item in the
                    transaction being a TID.
                </p>
                <p>At the first pass, the array is populated and the original database is transformed, and frequent
                    items are found. At subsequent passes, new candidate itemsets are formed and populates the array.
                    The support of each itemset is found by looping through each transaction. At each transaction, the
                    TIDs' extensions are checked, and if both the extensions' generators are in the transaction, it is
                    incremented and added to the transformed database. To filter for the frequent itemsets, loop through
                    the array and find which TIDs are frequent.</p>
            </section>
        </article>
    </main>
</body>

</html>
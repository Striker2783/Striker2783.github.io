<!DOCTYPE html>
<html>

<head>
    <title>Frequent Itemset Mining Algorithms</title>
    <link rel="stylesheet" href="/index.css" />
    <script src="/components/nav-bar.js"></script>
</head>

<body>
    <nav-bar></nav-bar>
    <main>
        <h1>Frequent Itemset Mining Algorithms</h1>
        <article>
            <section>
                <a href="https://github.com/Striker2783/data_mining/tree/master">Github Repository Link</a>
                <br>
                <a href="CS4710/paper.html">Multithreaded AprioriHybrid Paper</a>
                <h1>Introduction</h1>
                <p>This page shows a few algorithms for frequent itemset mining from a Introduction to Data Mining
                    Course I took in Spring 2025. I only needed to implement one of them.</p>
                <p>Frequent itemset mining is a process of identifying sets of items that appear frequently together in
                    a dataset. For example, if I have a dataset of shopping transactions, I can find what items were
                    commonly bought together. Using this data, I can find association rules that can be used to make
                    predictions about future behavior. Association rules determines the probability of someone buying
                    items based on what they bought previously. This is very useful in getting customers to buy more of
                    your products.</p>
            </section>
            <section>
                <h1>Algorithms</h1>
                <ul>
                    <li>
                        <a href="#apriori">Apriori</a>
                    </li>
                    <li>
                        <a href="#aprioritid">AprioriTID</a>
                    </li>
                    <li>
                        <a href="#apriorihybrid">AprioriHybrid</a>
                    </li>
                    <li>
                        <a href="#multithreaded-apriori">Multithreaded Apriori</a>
                    </li>
                    <li>
                        <a href="#multithreaded-apriorihybrid">Multithreaded AprioriHybrid</a>
                    </li>
                    <li>
                        <a href="#fp-growth">FP Growth</a>
                    </li>
                    <li>
                        <a href="#max-miner">Max Miner</a>
                    </li>
                    <li>
                        <a href="apriori-trie">Apriori with a Trie</a>
                    </li>
                </ul>
            </section>
            <section id="apriori">
                <h1>Apriori</h1>
                <p>The Apriori Algorithm is a fundamental method in the field of association rule mining, a branch of
                    data mining focused on discovering relationships among variables in large datasets. Introduced by
                    Agrawal and Srikant in 1994, it is designed to efficiently identify frequent itemsets and generate
                    association rules in transactional databases. Association rules are typically expressed in the form
                    "X implies Y," where X and Y are disjoint subsets of items. For example, in retail analytics, such
                    rules can reveal that customers who buy bread and butter are likely to also buy milk.
                </p>
                <p>
                    In the context of a transactional database, a transaction is defined as a set of items purchased
                    together, and an itemset is any subset of items. The support of an itemset measures how frequently
                    the itemset appears in the database, while the confidence of a rule quantifies the likelihood that a
                    transaction containing X also contains Y. An additional metric, lift, can assess the strength of an
                    association by comparing the observed frequency of X and Y occurring together to the frequency
                    expected if they were independent. Lift values greater than one indicate a positive correlation
                    between X and Y.
                </p>
                <p>
                    The algorithm relies on the Apriori property, also known as the downward-closure property, which
                    states that all subsets of a frequent itemset must themselves be frequent. This property allows the
                    algorithm to prune the search space efficiently: any itemset containing an infrequent subset can be
                    discarded from further consideration.
                </p>
                <p>
                    The algorithm proceeds in several iterative steps. Initially, it identifies all frequent individual
                    items that satisfy a minimum support threshold. These frequent items form the basis for generating
                    larger candidate itemsets. In each subsequent iteration, candidate itemsets of size k are generated
                    by combining frequent itemsets of size k - 1, and any candidate containing an infrequent subset is
                    pruned. The database is scanned to count the occurrences of each candidate, and those meeting the
                    minimum support requirement are retained as frequent itemsets. This iterative process continues
                    until no new frequent itemsets can be generated.
                </p>
                <p>
                    These candidate itemsets are stored in a data structure known as a Hash Tree. Each internal node of
                    the Hash Tree has an array of pointers (possibly null) to either other internal nodes or leaf nodes.
                    The leaf node contains a list of candidate itemsets and their support counts. When determining the
                    support of all the candidate itemsets, the Hash Tree takes in one transaction at a time and does a
                    recursive backtracking algorithm. At each internal node, it hashes each item in the transaction and
                    finds the node corresponding to that item. It then calls the recursive function on it. When it
                    reaches a leaf node, it increments the support of the itemset that was created by the items chosen.
                </p>
                <p>
                    After all frequent itemsets have been identified, the algorithm generates association rules. For
                    each frequent itemset, all possible non-empty subsets are considered, and rules are constructed by
                    treating one subset as the antecedent and the remaining items as the consequent. Each rule is
                    evaluated against a minimum confidence threshold, and only those that meet or exceed this threshold
                    are retained as strong rules.
                </p>
            </section>
        </article>
    </main>
</body>

</html>